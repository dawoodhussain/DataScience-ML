{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                 NATURAL LANGUAGE PROCESSING\n",
    "\n",
    "##### GLOB :\n",
    "with this package, we can read all filenames at once in a directory based on file masking (*)\n",
    "\n",
    "##### pd.concat:\n",
    "merging all the dataframes in a list format to one consolidated dataframe\n",
    "\n",
    "#### NLTK:\n",
    "This toolkit provides many functionalities like sentence tokenization, word tokenization, stop words, stemming, punctuations, etc..\n",
    "\n",
    "#### re:\n",
    "This library has functionalities to identify and remove punctuations and customized regular expressions\n",
    "\n",
    "### Tokenization:\n",
    "The process of converting paragraph into sentences OR sentences into bag of words is called Tokenization\n",
    "\n",
    "#### CountVectorizer:\n",
    "CountVectorizer implements both tokenization and count of occurrence of each word in a document.\n",
    "\n",
    "CountVectorizer just counts the word frequencies.\n",
    "\n",
    "COuntVectorizer gives higher weight to most commonly occurring words like 'an', 'the', 'is','there', etc.,\n",
    "\n",
    "#### TfidfVectorizer:\n",
    "TfidfVectorizer combines all options of CountVectorizer and TfidfTransformer in a single model.\n",
    "\n",
    "With the TFIDFVectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus. - This is the IDF (inverse document frequency part).\n",
    "\n",
    "This helps to adjust for the fact that some words appear more frequently.\n",
    "\n",
    "How often do you really use crepuscular, and petrichor?\n",
    "\n",
    "Sure, they are important words, and should be used when appropriate, but how often do we really use them?\n",
    "\n",
    "Without the inverse document frequency part commen, less meaning giving words such as “the” (assuming you don’t filter stopwords) would bear a higher weight than these less frequent words.\n",
    "\n",
    "Inverse document frequency is defined as:\n",
    "\n",
    "idf(t,D)=log{N|d∈D:t∈d|}\n",
    "\n",
    "Where N is the total number of documents in the corpus, and |d∈D:t∈d| is the number of documents where t appears.\n",
    "\n",
    "Today, people nearly always use the TFIDFVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time as time\n",
    "from glob import glob\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\msdaw\\\\Downloads\\\\YouTube-Spam-Collection-v1\\\\Youtube01-Psy.csv', 'C:\\\\Users\\\\msdaw\\\\Downloads\\\\YouTube-Spam-Collection-v1\\\\Youtube02-KatyPerry.csv', 'C:\\\\Users\\\\msdaw\\\\Downloads\\\\YouTube-Spam-Collection-v1\\\\Youtube03-LMFAO.csv', 'C:\\\\Users\\\\msdaw\\\\Downloads\\\\YouTube-Spam-Collection-v1\\\\Youtube04-Eminem.csv', 'C:\\\\Users\\\\msdaw\\\\Downloads\\\\YouTube-Spam-Collection-v1\\\\Youtube05-Shakira.csv']\n"
     ]
    }
   ],
   "source": [
    "filenames = glob(r'C:\\Users\\msdaw\\Downloads\\YouTube-Spam-Collection-v1\\Youtube*.csv')\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [pd.read_csv(f) for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS'], dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M.E.S                                              8\n",
       "Shadrach Grentz                                    7\n",
       "5000palo                                           7\n",
       "Louis Bryant                                       7\n",
       "DanteBTV                                           6\n",
       "LuckyMusiqLive                                     5\n",
       "Hidden Love                                        5\n",
       "Derek Moya                                         5\n",
       "RapStarz Coleman                                   4\n",
       "Laura Brown                                        4\n",
       "James Cook                                         4\n",
       "AllDailyVines                                      4\n",
       "Scott Johnson                                      4\n",
       "Pyles Baxter                                       3\n",
       "Adam Whitney                                       3\n",
       "Adam B                                             3\n",
       "ItsJoey Dash                                       3\n",
       "Ractive                                            3\n",
       "ricky swaggz                                       3\n",
       "deazy99                                            3\n",
       "Amir bassem                                        3\n",
       "OFFICIAL LEXIS                                     3\n",
       "Seth Ryan                                          3\n",
       "Drake Gagne                                        3\n",
       "Marshmallow Kingdom                                3\n",
       "DJ ROY                                             3\n",
       "roflcopter2110                                     3\n",
       "ThirdDegr3e                                        3\n",
       "macgyver16                                         3\n",
       "Jacob Johnson                                      3\n",
       "                                                  ..\n",
       "Linda Dipilato                                     1\n",
       "emrah England                                      1\n",
       "luisel Tutoriales Gameplays (luisel Tutoriales)    1\n",
       "rtfamily                                           1\n",
       "krayziejcs                                         1\n",
       "HEADSHOT946                                        1\n",
       "adrianavargova                                     1\n",
       "Chris Edgar                                        1\n",
       "keonte moore                                       1\n",
       "Daniel Brave                                       1\n",
       "breathinsoul                                       1\n",
       "Destiny Boatley                                    1\n",
       "Andrew Willigar                                    1\n",
       "MFkin PRXPHETZ                                     1\n",
       "devon smith                                        1\n",
       "Dj enderman                                        1\n",
       "bilal bilo                                         1\n",
       "sahir omran                                        1\n",
       "Tushti Chakraborty                                 1\n",
       "Eugene Kalinin                                     1\n",
       "Adele Lupei                                        1\n",
       "Beryllium                                          1\n",
       "Λυδία Γκ                                           1\n",
       "Misako Detenamo                                    1\n",
       "Alessio Siri                                       1\n",
       "FAHAD KHAN                                         1\n",
       "Dymetex                                            1\n",
       "nathan robson                                      1\n",
       "Ari4Cortez5Ortiz 9Uria                             1\n",
       "Mandeep Kaur                                       1\n",
       "Name: AUTHOR, Length: 1792, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['AUTHOR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>LneaDw26bFu8sZa1D5wQdex0wG1IYwFiZL4s3M0h2X8</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey its M.E.S here I&amp;#39;m a young up and comi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>LneaDw26bFsnJbhjejnJC_J6d5sHIH1B9UYVbAUc9KM</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im M.E.S an aspiring young rapper with high ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>LneaDw26bFvk4DAhUcCJKLzujguS_mf4eS_LdZjARzE</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey its M.E.S here I&amp;#39;m a young up and comi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>LneaDw26bFsMrQMk1vC-RxTxjmpFlt5sKz8Vo1_wIas</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey its M.E.S here I&amp;#39;m a young up and comi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>LneaDw26bFuADByLeh7RnEltROTIUCqeYYXmt51DT2g</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey its M.E.S here I&amp;#39;m a young up and comi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>LneaDw26bFtlox7jDN60_ys-XolAIlgwwc5y6aEKR68</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey guys i know its annoying getting spammed s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>LneaDw26bFuDsbyypF_jwmq7b6BqQPB7BdLbhfqBU5c</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey guys i know you wanna skip over this but p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>LneaDw26bFvn1m3oQLlCgsaxLcEy_eMQzcK9NAbyaew</td>\n",
       "      <td>M.E.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey guys look im aware im spamming and it piss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID AUTHOR DATE  \\\n",
       "1458  LneaDw26bFu8sZa1D5wQdex0wG1IYwFiZL4s3M0h2X8  M.E.S  NaN   \n",
       "1465  LneaDw26bFsnJbhjejnJC_J6d5sHIH1B9UYVbAUc9KM  M.E.S  NaN   \n",
       "1496  LneaDw26bFvk4DAhUcCJKLzujguS_mf4eS_LdZjARzE  M.E.S  NaN   \n",
       "1518  LneaDw26bFsMrQMk1vC-RxTxjmpFlt5sKz8Vo1_wIas  M.E.S  NaN   \n",
       "1526  LneaDw26bFuADByLeh7RnEltROTIUCqeYYXmt51DT2g  M.E.S  NaN   \n",
       "1542  LneaDw26bFtlox7jDN60_ys-XolAIlgwwc5y6aEKR68  M.E.S  NaN   \n",
       "1545  LneaDw26bFuDsbyypF_jwmq7b6BqQPB7BdLbhfqBU5c  M.E.S  NaN   \n",
       "1577  LneaDw26bFvn1m3oQLlCgsaxLcEy_eMQzcK9NAbyaew  M.E.S  NaN   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "1458  hey its M.E.S here I&#39;m a young up and comi...      1  \n",
       "1465  im M.E.S an aspiring young rapper with high ho...      1  \n",
       "1496  hey its M.E.S here I&#39;m a young up and comi...      1  \n",
       "1518  hey its M.E.S here I&#39;m a young up and comi...      1  \n",
       "1526  hey its M.E.S here I&#39;m a young up and comi...      1  \n",
       "1542  hey guys i know its annoying getting spammed s...      1  \n",
       "1545  hey guys i know you wanna skip over this but p...      1  \n",
       "1577  hey guys look im aware im spamming and it piss...      1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['AUTHOR']=='M.E.S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID      0\n",
       "AUTHOR          0\n",
       "DATE          245\n",
       "CONTENT         0\n",
       "CLASS           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CLASS</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CLASS\n",
       "CLASS    1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">CONTENT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>951</td>\n",
       "      <td>919</td>\n",
       "      <td>Like﻿</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1005</td>\n",
       "      <td>841</td>\n",
       "      <td>Check out this video on YouTube:﻿</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CONTENT                                               \n",
       "        count unique                                top freq\n",
       "CLASS                                                       \n",
       "0         951    919                              Like﻿    4\n",
       "1        1005    841  Check out this video on YouTube:﻿   97"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = df[['CONTENT','CLASS']]\n",
    "df_reduced.groupby('CLASS').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sentence into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['CONTENT']\n",
    "y = df['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import text processing libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2379)\t0.5312653407370687\n",
      "  (0, 735)\t0.46953667921795006\n",
      "  (0, 1047)\t0.41641989452600553\n",
      "  (0, 4277)\t0.311773937631005\n",
      "  (0, 1959)\t0.37285792450467403\n",
      "  (0, 2213)\t0.19506994634170918\n",
      "  (0, 2137)\t0.22272990824275116 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "print(X_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naives Bayes Classifier\n",
    "\n",
    "Again, scikit learn (python library) will help here to build a Naive Bayes model in Python. There are three types of Naive Bayes model under scikit learn library:\n",
    "\n",
    "**Gaussian:** It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "**Multinomial:** It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "**Bernoulli:** The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "Based on your data set, you can choose any of above discussed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()\n",
    "sgd = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultset = {}\n",
    "resultset['MultinomialNB before stopwords']= mnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X_train.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultset['GaussianNB before stopwords']= gnb.score(X_test.toarray(),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultset['BernoulliNB before stopwords']= bnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9468302658486708\n"
     ]
    }
   ],
   "source": [
    "resultset['SGDClassifier before stopwords']= sgd.score(X_test,y_test)\n",
    "print(sgd.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    3.0s finished\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\msdaw\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], 'n_iter': [500], 'loss': ['log'], 'penalty': ['l2'], 'n_jobs': [-1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "sgd = SGDClassifier()\n",
    "grid = {\n",
    "    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n",
    "    'n_iter': [500], # number of epochs\n",
    "    'loss': ['log'], # logistic regression,\n",
    "    'penalty': ['l2'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "grid_search = GridSearchCV(sgd,grid,verbose=1)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9468302658486708"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0]\n",
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "testing = ['checkout this link http://goahead','you have won 40 MIllion dollars','This trailer is awesome!']\n",
    "testing = tfidf.transform(testing)\n",
    "print(mnb.predict(testing))\n",
    "print(grid_search.best_estimator_.predict(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MultinomailNB() performs much better than GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       221\n",
      "           1       0.92      0.94      0.93       268\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       489\n",
      "   macro avg       0.93      0.92      0.93       489\n",
      "weighted avg       0.93      0.93      0.93       489\n",
      "\n",
      "accuracy : 0.9263803680981595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "y_pred = mnb.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('accuracy :',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's remove stopwords & see how the algorithms performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKENS</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Huh, ', 'anyway ', 'check ', 'you[tube] ', '...</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Hey ', 'guys ', 'check ', 'new ', 'channel '...</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['test ', 'I ', 'say ', 'murdev.com ']</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['shaking ', 'sexy ', 'ass ', 'channel ', 'enj...</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['watch?v=vtaRGgvGtWQ ', 'Check ', '.\\ufeff ']</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              TOKENS  \\\n",
       "0  ['Huh, ', 'anyway ', 'check ', 'you[tube] ', '...   \n",
       "1  ['Hey ', 'guys ', 'check ', 'new ', 'channel '...   \n",
       "2             ['test ', 'I ', 'say ', 'murdev.com ']   \n",
       "3  ['shaking ', 'sexy ', 'ass ', 'channel ', 'enj...   \n",
       "4     ['watch?v=vtaRGgvGtWQ ', 'Check ', '.\\ufeff ']   \n",
       "\n",
       "                                             CONTENT  \n",
       "0  Huh, anyway check out this you[tube] channel: ...  \n",
       "1  Hey guys check out my new channel and our firs...  \n",
       "2             just for test I have to say murdev.com  \n",
       "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿  \n",
       "4            watch?v=vtaRGgvGtWQ   Check this out .﻿  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TOKENS'] = df['CONTENT'].apply(lambda x: [item+' ' for item in x.split() if item not in stop_words])\n",
    "df['TOKENS'] = df['TOKENS'].astype('str')\n",
    "df[['TOKENS','CONTENT']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['TOKENS']\n",
    "y = df['CLASS']\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2370)\t0.5489550855086124\n",
      "  (0, 733)\t0.4851710211923769\n",
      "  (0, 1044)\t0.4302855866521627\n",
      "  (0, 2129)\t0.34971395087246654\n",
      "  (0, 1953)\t0.38527311709258943\n",
      "  (0, 4048)\t0.08588647169206838 1\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "print(X_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "gnb.fit(X_train.toarray(),y_train)\n",
    "bnb.fit(X_train,y_train)\n",
    "\n",
    "resultset['MultinomialNB after stopwords']= mnb.score(X_test,y_test)\n",
    "resultset['GaussianNB after stopwords']= gnb.score(X_test.toarray(),y_test)\n",
    "resultset['BernoulliNB after stopwords']= bnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MultinomialNB before stopwords': 0.9263803680981595,\n",
       " 'GaussianNB before stopwords': 0.787321063394683,\n",
       " 'BernoulliNB before stopwords': 0.8813905930470347,\n",
       " 'MultinomialNB after stopwords': 0.9243353783231084,\n",
       " 'GaussianNB after stopwords': 0.7832310838445807,\n",
       " 'BernoulliNB after stopwords': 0.820040899795501}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
